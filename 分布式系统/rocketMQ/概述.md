<img src="https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20211224224828693.png" alt="image-20211224224828693" style="zoom: 33%;" />

## 功能特性

### 普通消息

RocketMQ去发送消息有三种方式：可靠同步，可靠异步和单次的传输方式。消息总体可以分为两部分：消息内容和属性。消息内容即是需要投递的内容；属性可用于过滤，在生产者端定义，在消费者端可以通过sql表达式过滤。比如TAG就是一个特殊的属性。

### 顺序消息

对于一个指定的Topic，消息严格按照先进先出（FIFO）的原则进行消息发布和消费，即先发布的消息先消费，后发布的消息后消费。

顺序消息分为分区顺序消息和全局顺序消息

#### 全局顺序

生产者单线程，消费者单线程，只有一个队列。

性能与吞吐很差，只有极少场景需要这种消息，比如金融领域以及binlog消费

#### 分区顺序

只保证分区有序，不保证全局有序。生产者端采用Sharding Key 哈希取模进行分区，比如用户ID，订单ID等，这样可以保证同一订单的创建、支付、退货等消费在一个分区里有序。

性能较好，既保证业务的顺序，同时又能保证业务的高性能。

**问题：阿里云文档中的用于避免热点Key问题的逻辑分区、物理分区分别是什么？与queue有什么关系？**

### 延时/定时消息

- 定时消息：Producer将消息发送到消息队列RocketMQ版服务端，但并不期望立马投递这条消息，而是推迟到在当前时间点之后的某一个时间投递到Consumer进行消费，该消息即定时消息。
- 延时消息：Producer将消息发送到消息队列RocketMQ版服务端，但并不期望立马投递这条消息，而是延迟一定时间后才投递到Consumer进行消费，该消息即延时消息。

RocketMQ开源版只能设置固定级别的延迟。阿里云商业版支持40天内任意时间的延时

**问题：延时消息是如何实现的？和JDK的延时队列，Kafka的时间轮有什么区别？**

### 事务消息

RocketMQ采用半事务消息的方式支持类似XA/Open XA的分布式事务功能，能达到分布式事务的最终一致性。其大致交互如下：

<img src="https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/p365949.png" alt="事务消息" style="zoom:48%;" />

具体流程是在执行本地事务前先发送半事务消息，半事务消息为暂不可投递状态，不会发送给消费者。在执行本地事务后会根据事务的执行失败与否，发送Commit（TransactionStatus.CommitTransaction）或者Rollback（TransactionStatus.RollbackTransaction）消息，消息服务端根据提交状态判断消息发送与否。但是由于分布式系统的复杂性，网络之间的消息往往是三态的，即成功、失败以及超时。超时分为多种情况，比如请求超时、相应超时、Producer宕机等，或者是Producer端返回TransactionStatus.Unknow。消息服务端会启动消息回查，即主动去Producer端查询事务状态（推拉结合是保证数据一致性的一种常用套路，比如支付回调领域）。定时回查30秒一次。

### 批量消息

RocketMQ支持批量消息用于提升吞吐和消费端的性能，使用方式如下：

```java
String topic = "BatchTest";
List<Message> messages = new ArrayList<>();
messages.add(new Message(topic, "TagA", "OrderID001", "Hello world 0".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID002", "Hello world 1".getBytes()));
messages.add(new Message(topic, "TagA", "OrderID003", "Hello world 2".getBytes()));
try {
    producer.send(messages);
} catch (Exception e) {
    e.printStackTrace();
    //handle the error
}
```

### 消息重试

要想实现At Least Once语义，就必须要实现消息重试。

RocketMQ会为每个消费组都设置一个Topic名称为“%RETRY%+consumerGroup”的重试队列（这里需要注意的是，这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的），用于暂时保存因为各种异常而导致Consumer端无法消费的消息。考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时，重试次数越多投递延时就越大。RocketMQ对于重试消息的处理是先保存至Topic名称为“SCHEDULE_TOPIC_XXXX”的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至“%RETRY%+consumerGroup”的重试队列中。

#### 无序消息重试

对于无序消息而言，最大重试次数默认16次，可以通过**MaxReconsumeTimes**设置，重试间隔随着重试次数阶梯衰减，重试次数越大，重试间隔越长。

#### 顺序消息重试

对于有序消息，重试次数默认Integer.MAX，也可通过MaxReconsumeTimes设置，重试间隔通过**suspendTimeMillis**设置

### 消息过滤

rocketMQ支持多种形式的消息过滤，简单场景使用TAG过滤即可；复杂的可使用SQL过滤与自定义过滤器

#### tag过滤

常用模式，可通过TAG过滤，可订阅一个也可订阅多个，可发送一个也可发送多个。也可通过SQL方式设置TAG，默认属性key为TAGS。

后续消费者队列中会携带TAG的hashcode也区分tag，由于hashcode可能重复，在消费者端消费时会再次比较tag的值。

- 发送消息

  发送消息时，每条消息必须指明Tag。

  ```java
      Message msg = new Message("MQ_TOPIC","TagA","Hello MQ".getBytes());                
  ```

- 订阅所有Tag

  消费者如需订阅某Topic下所有类型的消息，Tag用星号（*）表示。

  ```java
      consumer.subscribe("MQ_TOPIC", "*", new MessageListener() {
          public Action consume(Message message, ConsumeContext context) {
              System.out.println(message.getMsgID());
              return Action.CommitMessage;
          }
      });                
  ```

- 订阅单个Tag

  消费者如需订阅某Topic下某一种类型的消息，请明确标明Tag。

  ```java
      consumer.subscribe("MQ_TOPIC", "TagA", new MessageListener() {
          public Action consume(Message message, ConsumeContext context) {
              System.out.println(message.getMsgID());
              return Action.CommitMessage;
          }
      });                
  ```

- 订阅多个Tag

  消费者如需订阅某Topic下多种类型的消息，请在多个Tag之间用两个竖线（||）分隔。

  ```java
      consumer.subscribe("MQ_TOPIC", "TagA||TagB", new MessageListener() {
          public Action consume(Message message, ConsumeContext context) {
              System.out.println(message.getMsgID());
              return Action.CommitMessage;
          }
      });                
  ```

#### sql过滤

- 消息发送端：

  设置消息的自定义属性。

  ```java
  Message msg = new Message("topic", "tagA", "Hello MQ".getBytes());
  // 设置自定义属性A，属性值为1。
  msg.putUserProperties("A", "1");
  ```

- 消息消费端

  使用SQL语法设置过滤表达式，并根据自定义属性过滤消息。

  **注意** 使用属性时，需要先判断属性是否存在。若属性不存在则过滤表达式的计算结果为NULL，消息不会被投递到消费端。

  ```typescript
  // 订阅自定义属性A存在且属性值为1的消息。
  consumer.subscribe("topic", MessageSelector.bySql("A IS NOT NULL AND TAGS IS NOT NULL AND A = '1'"), new MessageListener() {
      public Action consume(Message message, ConsumeContext context) {
          System.out.printf("Receive New Messages: %s %n", message);
          return Action.CommitMessage;
      }
  });
  ```

#### 自定义过滤器过滤

rocketMQ支持扩展SPI，可自定义过滤器，该过滤器会被上传到Broker中，进行远程代码执行

### 消费模式

rocketMQ支持集群消费和广播消费两种模式，默认集群消费，这也是常用的一种方式。

#### 集群消费

集群消费是常用的一种消费方式，该方式中消费会在消费者组中轮询消费，每个消费者消费一次。但是消费者组之间一般默认广播消费，即消费会在每个消费者组都投递一次。

#### 广播消费

该场景再消费者组里消费者之间用的不多，一般只有消费者组之间使用该模式。但是有些特殊场景下使用该模式可能会有出奇的效果。比如大促时redis的并发可能会成为一个瓶颈，此时可通过本地缓存来去掉redis，而整个集群中缓存通过就可以使用广播模式来保证整个集群的缓存一致性。

需要注意rocketMQ广播模式下消息不会重试

## 高可用架构

rocketMQ采用的是主从同步的高可用架构，这点与Kafka不同。分为单Master模式、多Master模式、多Master多slave模式等多种模式。整体架构如下

![image-20220921221806378](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20220921221806378.png)

图中所涉及到的概念如下所述：

- Name Server：是一个几乎无状态节点，可集群部署，在消息队列RocketMQ版中提供命名服务，更新和发现Broker服务。NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。

- Broker：消息中转角色，负责存储消息，转发消息。分为Master Broker和Slave Broker，一个Master Broker可以对应多个Slave Broker，但是一个Slave Broker只能对应一个Master Broker。Broker启动后需要完成一次将自己注册至Name Server的操作；随后每隔30s定期向Name Server上报Topic路由信息。
- 生产者：与Name Server集群中的其中一个节点（随机）建立长连接（Keep-alive），定期从Name Server读取Topic路由信息，并向提供Topic服务的Master Broker建立长连接，且定时向Master Broker发送心跳。
- 消费者：与Name Server集群中的其中一个节点（随机）建立长连接，定期从Name Server拉取Topic路由信息，并向提供Topic服务的Master Broker、Slave Broker建立长连接，且定时向Master Broker、Slave Broker发送心跳。Consumer既可以从Master Broker订阅消息，也可以从Slave Broker订阅消息，订阅规则由Broker配置决定。

4.5之前没有主从切换的容灾能力，4.5之后**Dledger**集群架构实现了master的自动选举，并提升了主从复制的性能--半数以上ack即成功，避免了最慢响应。

## 存储架构

rocketMQ对Kafka的存储架构做了优化，采用CommitLog将随机写变更为顺序写，类似于Mysql的redoLog，都是一种WAL（Write Ahead Log 预写日志）技术。每台Broker只有一个CommitLog，broker上的所有queue共用，这样在提升了写磁盘性能之外，也实现了quere的横向扩展。加队列不会影响rocketMQ的写入性能，这点与Kafka不同，Kafka是每个Partition都有一个日志文件。queue中存储的只是CommitLog中的offset（可以认为是索引），真正的数据存储在CommitLog中。其架构如下：

![img](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/rocketmq_design_1.png)

同时它采用了零拷贝的方式将数据直接写入内核态，避免了用户态线程和内核态线程之间的数据交换开销。

其中ConsumerQueue采用的是一种定长结构，理论上可无限扩展，结构如下

![img](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/rocketmq_design_7.png)

## 面试题

#### 1、如何保证消息不丢

主要有如下四个环节有可能丢消息。

![image-20220921221833531](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20220921221833531.png)

- 1,4两个环节，主要通过重试与ack确认机制，如果需要更严格的消息不丢，比如MQ集群整个挂掉--磁盘损坏，可通过事务消息、本地消息表等分布式事务解决方案
- 2环节丢消息，可通过开启同步复制避免，但是性能会降低。4.5之后**Dledger**集群架构可提升主从复制性能与可用性。
- 3环节丢消息，可通过同步刷盘避免

#### 2、消息积压如何处理

消息积压一般有如下三个个原因：

- 下游服务错误导致消息重试
- 下游消费者消费能力不足（集群太小或者消费者线程设置不合理），不能大于队列数，大于队列数的消费者无法消费
- broker队列数不足，扩容队列数，同时要扩容消费者、

视消息积压严重程度可采取不同的处理措施。

1. 如果消息积压刚开始，并不严重

   这种情况可以先解决下游问题，如果是异常导致，则修复消费者异常，上线即可。如果是下游消费者能力不足，则扩容消费者即可。如果是队列数不足，则扩容队列数，同时扩容消费者集群或者线程即可

2. 如果消息积压严重

​		此时需要先处理消息，再修复异常。可以通过如下方法：

	- 如果消息不重要，可直接移动偏移量offset，丢弃消息
	- 如果消息重要，但是生产者有重放能力，此时也可以直接丢弃消息，待问题修复后，再重放消息
	- 如果消息重要，并且生产者没有重放能力，此时就需要找个地方暂存消息。可以新建一个新的消费者组，不做任何逻辑处理，将消息暂存到DB或者一个队列更多的TOPIC中。待问题修复后再消费。

3. 如果问题修复了，但是消息积压非常严重，如果快速消费消息，减少故障时间 -- 即当前业务不容忍消息慢慢消费
   - 可通过创建一个更多队列的TOPIC来替换老的TOPIC。老topic写入新topic时不做任何逻辑，速度较快。新topic在消费时有之前逻辑，但是队列更多，消费者线程更多，因此会处理更快。

#### 3、 如果保证消息有序

###### 全局有序

一个生产者+一个队列+一个消费者

###### 局部有序

两种方式

1. 通过MessageSelector对象指定队列

2. 通过shardingKey哈希取模，shardingKey可取同一个用户或者同一个订单

   这种情况下如何解决扩容缩容问题？可通过加一层逻辑队列，逻辑队列固定不变，物理队列课随意扩缩容，生产者消费者与逻辑队列交互，需要维护逻辑队列到物理队列的映射。

   ![image-20220921221853831](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20220921221853831.png)

## 参考链接

[https://yuya008.gitbooks.io/apache-rocketmq/content/chapter1/bu-shu.html](https://yuya008.gitbooks.io/apache-rocketmq/content/chapter1/bu-shu.html)

[https://zhuanlan.zhihu.com/p/94662788](https://zhuanlan.zhihu.com/p/94662788)

[https://help.aliyun.com/document_detail/112010.html](https://help.aliyun.com/document_detail/112010.html)

[https://rocketmq.apache.org/docs/quick-start/](https://rocketmq.apache.org/docs/quick-start/)

