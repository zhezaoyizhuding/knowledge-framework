[TOC]

## 一.背景

在众多的互联网业务中，往往需要对大量的数据进行唯一的标识。如视频类App，需要对每一个视频进行唯一的ID标识；知乎每一篇回答每一个问题需要唯一标记；IM通信类App需要唯一标记每一条消息；购物类App需要唯一标记每一个商品，每一笔订单等等。

因为有以上场景，就有了发号器的需求，即一个生成唯一ID的服务，而这个服务一般调用量非常大，跟视频数，回答数，订单数等是正相关的。在现在动辄日活过亿的App面前，不可能是单机提供服务，只能是分布式部署。任何一个技术问题，在海量请求面前都会变得复杂起来。

发号器作为最基本的服务，需要具有以下基本能力：

1. 全局唯一或单用户唯一。在订单系统等场景下要求全局唯一，在IM场景下，在单用户维度下保持唯一即可。
2. 高性能，高可用。发号服务一般是最基础的服务，是业务系统最核心的环节，动辄每天亿级调用，希望无限趋近于100%的可用性，并且保证低延迟。

除了最基本的能力外，根据业务的特点，对发号器会有以下要求：

1. 趋势递增。 大多数场景下，生成的ID需要在Mysql等RDBMS数据库存储，并且作为索引，一般使用B+树的存储结构，所以在ID的生成上，最好保证是递增的，这样可以提高写入的性能。
2. 严格递增。排序，IM增量消息等场景，会根据消息的ID做一致性和可靠性保证，比如IM消息，会将本地存储的ID和服务端存储的ID做对比，发现落后于服务端之后就会拉取新消息。这个时候就要保证严格递增，不能回退，否则就会导致消息错乱。
3. ID防遍历。有些场景下，是需要避免ID被遍历的，否则造成内部信息的泄漏，比如如果活动ID是递增的，就可以通过脚本遍历活动信息，进行刷量，漏洞攻击等。或者是根据递增的订单号，就可以分析一天增加了多少订单，被竞争对手利用。虽然拿到ID后业务可以进行加密处理，但这无疑是对业务层提出了更高的要求，提高了风险。

1、2 和 3是矛盾的，做不到兼顾，所以一般使用不同的ID生成策略。

## 二.发号器生成基础方法

### 1.数据库自增ID

利用数据库的 auto_increment 和 auto_increment_offset 实现自增 ID，当需要生成一个ID的时候，向表中插入一条记录返回主键 ID。

优点：

- 简单，利用数据库的系统功能实现，代码也非常简洁，有专门的DBA对数据库进行维护。
- 生成的数据是递增的，在大多数业务场景下是比较友好的。

缺点：

- 数据库本身读写存在性能瓶颈，单点DB在高并发场景下难以支撑。
- 强依赖DB，DB异常时整个服务不可用，虽然可以配置主从复制提高可用性，但主从切换时数据一致性难以保障，在不一致时可能出现ID重复的场景。

优化：

为了提高性能，可以采用多机部署的方案，整体的架构图如下：

![img](https://pic2.zhimg.com/80/v2-eee1304749fe792e7c7d4a27029dea31_720w.webp)

多机部署的DB发号器

通过多机部署DB，解决性能问题，根据机器数设置DB步长，N台DB机，则设置步长为N，这样每个DB发放的序列都是不同的。虽然解决了性能问题，但也带来了其他问题：

- 扩展能力差，缩容时，不需要调整，但是如果需要扩容，比如增加一台机器，这个时候所有DB的步长都需要改变成N+1，需要将所有DB服务变更一遍，并且每个DB的起始值都要满足新的步长结果，还需要停机操作。这样扩容时非常复杂且容易出问题的。
- 虽然针对扩容能力差，可以将步长设置的比机器数大一些，这样扩容时就不需要调整其他DB的步长了，但会造成ID大幅回退。
- 自增ID不再是严格递增的，而是趋势递增的，比如第一次拿到0，第二次是N+0， 第三次是1，ID出现了回退，在要求严格递增的场景下就不适用了。
- 每次获取ID都需要写DB，对DB的压力还是很大，只能靠堆机器解决，成本比较高。

### 2.UUID

**通用唯一识别码**（英语：**U**niversally**U**nique**Id**entifier，缩写：**UUID**）是用于计算机体系中以识别信息的一个128位标识符，是由一个16进制下的32位数所构成，故UUID理论上的总数为 1632 ，约等于3×1038 。也就是说若每纳秒（ns）产生1万亿个UUID，要花100亿年才会将所有UUID用完。UUID的标准型式包含32个16进制数字，以连字号分为五段，形式为 8-4-4-4-12 的32个字符。示例：

```text
550e8400-e29b-41d4-a716-446655440000 
```

业界一共有五个版本的生成方案。

优点：

- 不依赖中心式的注册和分配，本地生成，不需要做持久化，性能非常好。
- 主流语言都有现成的库，代码实现起来也非常简单。

缺点：

- 字段长度大，不易存储。UUID16字节128位，不能使用32位或64位数字存储，通常以36长度的字符串表示。
- 无序，非递增，不利于作为索引进行存储。
- 版本3，4，5有一定的冲突概率，虽然概率很小，但生成量非常大的情况下还是有可能重复。
- 版本1通常使用MAC地址生成，有一定的信息泄漏风险。

### 3.雪花算法

雪花算法（Snowflake），twitter公司开源的算法，思想是将一个64位数据分段，分别标识时间，机器ID，自增序列号等，如图所示，采用41位表示时间（ms），可以表示69年时间，10bit可以表示出1024台机器，12位递增ID可以表示出4096个ID，根据这个划分，1毫秒每台机器可以生成4096个ID，QPS可以达到400w/s，可以满足大部分场景的需求。

![img](https://pic1.zhimg.com/80/v2-0a0d7f608023c3ba808064029df0d9e8_720w.webp)

根据雪花算法，有非常多类似的ID生成方式，比如腾讯视频的视频ID，也是分段进行表示的，只不过不是纯数字，也包括字母的组合

优点：

- 高位表示毫秒数，整个发号器是趋势递增的。
- 不依赖任何第三方存储，不需要持久化，稳定性高。
- 可以根据业务需求灵活的分配每段的长度。

缺点：

- 强依赖机器时钟，如果机器时钟回拨，将会出现相同的ID

## 三.大型互联网公司的发号器

互联网公司，因为用户量大，并发高，并且涉及到订单，IM等核心业务，对发号器的要求更高，并且业务的特点有所差异，所以每家互联网公司都会做一个轮子出来， 下面针对一些比较有特点的设计，做简要的介绍。

### 1.双buffer DB 发号器

美团和滴滴采用了这种方案，本质上还是使用数据库递增的方式，DB自增的方式，实际上是每次新增ID都将数据进行持久化，但存储服务的写性能一边情况下是比较差的，是服务的瓶颈，通过减少写DB的次数来提升性能。

每个服务器上缓存一个号段，发号时，先从缓存中取，等缓存中取完了，就更新DB中号段的最大值，同时更新缓存的号段，如图，三台机器分别缓存了号段A：1-1000，B：1001-2000，C：2001-3000，这时候DB 中存储的号段最大值为3000， 这个时候A机器的号段发完了，这个时候要更新DB最号段最大值位4000，并更新A机器的号段为3001-4000。这样就可以将读写DB的次数降为1/step， step位号段的长度。

![img](https://pic1.zhimg.com/80/v2-659436e8f2b5c31a010586e1985b9d50_720w.webp)

如果在发号的过程中服务挂了，缓存的号段丢失了，当服务再拉起时，就更新DB的最大值，重新取号段，对外表现就是一段号码被跳过了，也没有大的影响。

上述方案是单Buffer，当号码分配完了再去更新DB重新分配号段，在高并发场景下必然会造成服务的短暂不可用，因为读写DB的IO延时是比较大的，会造成TP999数据波动大。所以可以在号段还未发完时就预先分配号段，但不能直接覆盖原有的号段，因为还没有分配完，这就需要另一个Buffer了。

![img](https://pic3.zhimg.com/80/v2-a220a39ecc2a2be6b3f37e594c275f96_720w.webp)

服务内部有两个号段缓存区segment。当前号段已下发10%时，每次请求都判断下下一个号段是否更新，如果未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复， 这样可以消除持久化DB造成的服务抖动。

双Buffer方案通过拆分号段，减少持久化操作，实现了高性能，但ID生成时趋势递增的，而不是单调递增的，因为两次请求落在不同的机器上就可能造成ID回退，如果要求严格递增，这种方案必须保证单业务落在同一台服务器上，从而无法保证负载均衡。有赞团队实现的March发号器就是严格递增的。

对于IM场景，对于每个用户消息，需要一个唯一的ID进行标记，但用户之间是可以重复的，这个时候就相当于用户数个业务，每个业务都要分配自增ID，而且这个ID必须是严格递增的。对于这种业务场景，存在以下两个问题：

- 如果每个用户存储一个当前最大ID，那对于过亿用户将是GB级的存储，在服务重启加载号段会非常耗时，并且不可靠；
- 虽然单个用户获取递增ID的频率不高，但用户太多了，还是会对DB有超多的写操作，存在瓶颈。

针对上面的问题，可以将多个用户共用一个号段上限，任意一个用户到达上限了就更新持久化的号段上限，并且将通过多个用户组合成一个group，并通过一致性哈希算法进行进行负载均衡分配。比如如果10w用足组成一个group，那用户max_id存储大小将降至KB级别，重启加载就可以接受。

![img](https://pic1.zhimg.com/80/v2-d4fb5f98097302509e03c57dd8cd20b0_720w.webp)

### 2. 雪花算法优化

雪花算法需要解决两个问题1. 机器可能存在时钟回拨；2.机器（workerid）的分配管理

1. 借助中心化的节点

通过建设中心化的节点，如Zookeeper来维护每台机器的状态和时钟信息。以美团leaf框架为例，架构如图

![img](https://pic3.zhimg.com/80/v2-9f1169a89670e407794fba1a64f3240e_720w.webp)

- 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。
- 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。
- 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。

在解决时钟问题上，分成两种类型解决

服务启动回拨：通过定时上报本机的时钟信息，在第一次启动服务时（未上报过）如果发现自己的时钟信息与其他所有机器的平均时钟信息相差过大，则认为有问题，启动失败。 如果有上报记录，则对比这次时钟远小于最后一次上报时钟的差异，则认为时钟进行了回拨，这个时候告警。

服务运行中回拨：运行中回拨，会导致发号过程中时间戳变小，这个时候等待一定时间再提供服务，若多次回拨则剔除机器并告警，本质是重试并等待时钟追赶。

2.历史时钟方式

将序列号的分配调整为如图，8bit序列号高位和最后1bit序列号低位，将其中“机器id”调整到最后，是为了避免“序列号”增1导致的整体数据增1的问题，这样可以在一定程度上规避外部数据对id的猜测，以防止恶意爬取。

将序列号拆分为8bit和1bit是为了防止总是生成奇数或者偶数的问题。

![img](https://pic4.zhimg.com/80/v2-2cbd6fb751b553011e793830716c8033_720w.webp)

41bit时间戳不表示当前时间戳，而是在启动时初始化为当前时间戳，然后根据序列号累计进行递增，序列号自增到最大值时候时间戳增1，而序列号重新归为0，当然限制时间戳不能超过当前时间戳，也就是说当分配速率超快时，就退化为传统的雪花算法。

在服务重新启动时，41bit的时间戳又会更新为当前时间，避免出现重复的ID。

采用历史时间则天然的不存在时间回拨问题。但是在超高并发情况下，历史的时间很快用完，时间一直保持在最新时间的话，这个时候出现时间回拨，则采用业界对于时间回拨的处理方式（首次等待，即等待一段回拨时间）

这个方案还可以短暂拉升性能，因为在QPS小于最高速度时，一段时间后，时间戳就落后于当前时间，这个时候业务请求突增，因为时间戳距离当前时刻较远，就可以产生更多的ID，非常适合于平均QPS不高，但是有突增流量的场景。

![img](https://pic1.zhimg.com/80/v2-99d8a01557687f2279e028d40b27be9c_720w.webp)

## 四.总结

本文主要介绍了发号器的背景，常见的生成方式，以及一些互联网大厂在高并发，高可靠性要求下所做的一些优化。最后再来个表格做对比。

| 方案         | 优点                                                         | 缺点                                                         | 优化点                                    |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------- |
| 数据库自增ID | 1，简单，利用数据库的系统功能实现，代码也非常简洁，有专门的DBA对数据库进行维护。  2.生成的数据是递增的，在大多数业务场景下是比较友好的。 | 1，数据库本身读写存在性能瓶颈，单点DB在高并发场景下难以支撑。  2.强依赖DB，DB异常时整个服务不可用，主从复制可以提高可用性，但数据一致性难以保障，可能出现ID重复的场景。 | 1.多机部署，分号段发放 2.双Buffer缓存优化 |
| UUID         | 1.不依赖中心式的注册和分配，本地生成，不需要做持久化，性能非常好。  2.主流语言都有现成的库，代码实现起来也非常简单。 | 1.字段长度大，不易存储。UUID16字节128位，不能使用32位或64位数字存储，通常以36长度的字符串表示。  2. 无序，非递增，不利于作为索引进行存储。  3. 版本3，4，5有一定的冲突概率，虽然概率很小，但生成量非常大的情况下还是有可能重复。  4. 版本1通常使用MAC地址生成，有一定的信息泄漏风险。 |                                           |
| 雪花算法     | 1. 高位表示毫秒数，整个发号器是趋势递增的。  2.不依赖任何第三方存储，不需要持久化，稳定性高。  3. 可以根据业务需求灵活的分配每段的长度。 | 1.强依赖机器时钟，如果机器时钟回拨，将会出现相同的ID         | 1.中心化节点进行管理  2.采用历史时钟方式  |

## 五.参考文献

> [如何做一个靠谱的发号器](https://link.zhihu.com/?target=https%3A//tech.youzan.com/id_generator/)
> [Leaf——美团点评分布式ID生成系统](https://link.zhihu.com/?target=https%3A//tech.meituan.com/2017/04/21/mt-leaf.html)
> [https://github.com/didi/tinyid](https://link.zhihu.com/?target=https%3A//github.com/didi/tinyid)
> [https://github.com/baidu/uid-generator](https://link.zhihu.com/?target=https%3A//links.jianshu.com/go%3Fto%3Dhttps%3A%2F%2Fgithub.com%2Fbaidu%2Fuid-generator)
> [改进版雪花设计 · 语雀](https://link.zhihu.com/?target=https%3A//www.yuque.com/simonalong/butterfly/mn26oy
>
> https://zhuanlan.zhihu.com/p/493375661

- 
