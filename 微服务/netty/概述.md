## 线程模型

<img src="https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004111131941.png" alt="image-20221004111131941"  />

![image-20221004114547723](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004114547723.png)

- Netty 抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接, WorkerGroup专门负责网络的读写 

- BossGroup和WorkerGroup类型都是NioEventLoopGroup 

- NioEventLoopGroup 相当于一个事件循环**线程组**, 这个组中含有多个事件循环线程 ， 每一个事件循环线程是NioEventLoop 

- 每个NioEventLoop都有一个selector , 用于监听注册在其上的socketChannel的网络通讯 

- 每个Boss NioEventLoop线程内部循环执行的步骤有 3 步 
  - 处理accept事件 , 与client 建立连接 , 生成 NioSocketChannel
  - 将NioSocketChannel注册到某个worker NIOEventLoop上的selector 
  - 处理任务队列的任务 ， 即runAllTasks 

- 每个worker NIOEventLoop线程循环执行的步骤，

  - 轮询注册到自己selector上的所有NioSocketChannel 的read, write事件
  - 处理 I/O 事件， 即read , write 事件， 在对应NioSocketChannel 处理业务 

  - runAllTasks处理任务队列TaskQueue的任务 ，一些耗时的业务处理一般可以放入TaskQueue中慢慢处理，这样不影响数据在 pipeline 中的流动处理 

- 每个worker NIOEventLoop处理NioSocketChannel业务时，会使用 pipeline (管道)，管道中维护了很多 handler 处理器用来处理 channel 中的数据 

## 功能组件

**【Bootstrap、ServerBootstrap】**

Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。 

**【Future、ChannelFuture】** 

正如前面介绍，在 Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注 册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。 

**【Channel】：** 

Netty 网络通信的组件，能够用于执行网络 I/O 操作。Channel 为用户提供： 

1. 当前网络连接的通道的状态（例如是否打开？是否已连接？） 

2. 网络连接的配置参数 （例如接收缓冲区大小） 

3. 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成。 

4. 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方。 

5. 支持关联 I/O 操作与对应的处理程序。不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应。 

下面是一些常用的 Channel 类型： 

- NioSocketChannel，异步的客户端 TCP Socket 连接。 

- NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 

- NioDatagramChannel，异步的 UDP 连接。 

- NioSctpChannel，异步的客户端 Sctp 连接。 

- NioSctpServerChannel，异步的 Sctp 服务器端连接。 

- 这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。 

**【Selector】：** 

Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。 当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询(Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 。 

**【NioEventLoop】：** 

NioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的 run 方法，执行 I/O 任务和非 I/O 任务： 

- I/O 任务，即 selectionKey 中 ready 的事件，如 accept、connect、read、write 等，由 processSelectedKeys 方法触发。 

- 非 IO 任务，添加到 taskQueue 中的任务，如 register0、bind0 等任务，由 runAllTasks 方法触发。 

**【NioEventLoopGroup】：** 

NioEventLoopGroup，主要管理 eventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个 Channel 上的事件，而一个 Channel 只对应于一个线程。 

**【ChannelHandler】：** 

ChannelHandler 是一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到其 ChannelPipeline(业务处理链)中的下一个处理程序ChannelHandler 本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类： 

- ChannelInboundHandler 用于处理入站 I/O 事件。 

- ChannelOutboundHandler 用于处理出站 I/O 操作。

或者使用以下适配器类：

- ChannelInboundHandlerAdapter 用于处理入站 I/O 事件。 

- ChannelOutboundHandlerAdapter 用于处理出站 I/O 操作。 

**【ChannelHandlerContext】：** 

保存 Channel 相关的所有上下文信息，同时关联一个 ChannelHandler 对象。 

**【ChannelPipline】：** 

保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站事件和出站操作。ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互。在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下： 

![image-20221004112311110](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004112311110.png)

一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。read事件(入站事件)和write事件(出站事件)在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。

**编码解码器** 

当你通过Netty发送或者接受一个消息的时候，就将会发生一次数据转换。入站消息会被**解码**：从字节转换为另一种格式（比如java对 象）；如果是出站消息，它会被**编码成字节**。Netty提供了一系列实用的编码解码器，他们都实现了ChannelInboundHadnler或者ChannelOutboundHandler接口。在这些类中，channelRead方法已经被重写了。以入站为例，对于每个从入站Channel读取的消息，这个方法会被调用。随后，它将调用由已知解码器所提供的decode()方法进行解码，并将已经解码的字节转发给ChannelPipeline中的下一个ChannelInboundHandler。Netty提供了很多编解码器，比如编解码字符串的StringEncoder和StringDecoder，编解码对象的ObjectEncoder和ObjectDecoder等。

如果要实现高效的编解码可以用protobuf，但是protobuf需要维护大量的proto文件比较麻烦，现在一般可以使用protostuff。protostuff是一个基于protobuf实现的序列化方法，它较于protobuf最明显的好处是，在几乎不损耗性能的情况下做到了不用我们写.proto文件来实现序列化。

**粘包拆包**

TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区 的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成 一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。如下图所示，client发了两个数据包D1和D2，但是server端可能会收到如下几种情况的数据。

![image-20221004113756454](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004113756454.png)

**解决方案** 

- 消息定长度，传输的数据大小固定长度，例如每段的长度固定为100字节，如果不够空位补空格 
- 在数据包尾部添加特殊分隔符，比如下划线，中划线等，这种方法简单易行，但选择分隔符的时候一定要注意每条数据的内部一定不 能出现分隔符。 

- 发送长度：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4位是数据的长度，应用层处理时可以根据长度来判断每条数据的开始和结束。 

Netty提供了多个解码器，可以进行分包的操作，如下： 

- LineBasedFrameDecoder （回车换行分包） 

- DelimiterBasedFrameDecoder（特殊分隔符分包） 

- FixedLengthFrameDecoder（固定长度报文来分包）

**Netty心跳检测机制**

**Netty断线自动重连实现**

## ByteBuf

从结构上来说，ByteBuf 由一串字节数组构成。数组中每个字节用来存放信息。ByteBuf 提供了两个索引，一个用于读取数据，一个用于写入数据。这两个索引通过在字节数组中移动，来定 位需要读或者写信息的位置。当从 ByteBuf 读取时，它的 readerIndex（读索引）将会根据读取的字节数递增。同样，当写 ByteBuf 时，它的 writerIndex 也会根据写入的字节数进行递增。

![image-20221004112841250](https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004112841250.png)

#### 扩容机制

当writerIndex达到capacity的时候，再往里面写入内容，ByteBuf就会进行扩容。

<img src="https://yusheng-picgo.oss-cn-beijing.aliyuncs.com/picgo/image-20221004155812840.png" alt="image-20221004155812840" style="zoom:50%;" />

- minNewCapacity：表用户需要写入的值大小 

- threshold：阈值，为Bytebuf内部设定容量的最大值 

- maxCapacity：Netty最大能接受的容量大小，一般为int的最大值 

扩容流程：

1. 默认门限阈值为4MB(这个阈值是一个经验值，不同场景，可能取值不同)，当需要的容量等于门限阈值，使用阈值作为新的缓存区容量。
2. 如果大于阈值，采用每次步进4MB的方式进行内存扩张（(需要扩容值/4MB)*4MB），扩张后需要和最大内存（maxCapacity）进行比较，大于maxCapacity的话就用maxCapacity,否则使用扩容值目标容量。
3. 如果小于阈值，采用倍增的方式，以64（字节）作为基本数值，每次翻倍增长64 -->128 --> 256，直到倍增后的结果大于或等于需要的容量值。 

## 参考文档

